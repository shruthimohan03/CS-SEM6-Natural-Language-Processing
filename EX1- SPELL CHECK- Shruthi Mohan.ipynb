{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8568051e",
   "metadata": {},
   "source": [
    "### Trie class: Insertion and Searching functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663b5422",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    # Trie node class\n",
    "    def __init__(self):\n",
    "        self.children = [None] * 128  # Assuming ASCII characters\n",
    "        self.wordCount = 0\n",
    "        self.isEndOfWord = False\n",
    "\n",
    "    def insert_key(self, key):\n",
    "        # Initialize the currentNode pointer with the root node\n",
    "        currentNode = self\n",
    "        \n",
    "        # Iterate across the length of the string\n",
    "        for c in key:\n",
    "            index = ord(c)\n",
    "            if currentNode.children[index] is None:\n",
    "                # If node for current character does not exist then make a new node\n",
    "                newNode = TrieNode()\n",
    "                # Keep the reference for the newly created node.\n",
    "                currentNode.children[index] = newNode\n",
    "            \n",
    "            # Now, move the current node pointer to the newly created node.\n",
    "            currentNode = currentNode.children[index]\n",
    "        \n",
    "        # Increment the wordEndCount for the last currentNode pointer. \n",
    "        # This implies that there is a string ending at currentNode.\n",
    "        currentNode.wordCount += 1\n",
    "        currentNode.isEndOfWord = True\n",
    "\n",
    "    def is_prefix_exist(self, key):\n",
    "        # Initialize the currentNode pointer with the root node\n",
    "        current_node = self\n",
    "        \n",
    "        # Iterate across the length of the string\n",
    "        for c in key:\n",
    "            # Check if the node exists for the current character in the Trie.\n",
    "            index = ord(c)\n",
    "            if current_node.children[index] is None:\n",
    "                # Given word as a prefix does not exist in Trie\n",
    "                return False\n",
    "            \n",
    "            # Move the currentNode pointer to the already existing node for the current character.\n",
    "            current_node = current_node.children[index]\n",
    "        \n",
    "        # Prefix exists in the Trie\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117f8fa",
   "metadata": {},
   "source": [
    "### Installing 'words' package from 'corpus' library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b477a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc83f4a",
   "metadata": {},
   "source": [
    "### Importing the 'words' package (corpus/ dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3660b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f6c8f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6666638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "root = TrieNode()\n",
    "for word in words.words():\n",
    "    root.insert_key(word)\n",
    "\n",
    "print(root.is_prefix_exist('play'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7465e",
   "metadata": {},
   "source": [
    "### Input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd3d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence='During the summer we have the best ueather. I have a black ueather jacket, so nice.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb2726",
   "metadata": {},
   "source": [
    "# TEXT PREPROCESSING (FOR ONE INPUT SENTENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea948ce5",
   "metadata": {},
   "source": [
    "### Convert text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80ee9cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'during the summer we have the best ueather. i have a black ueather jacket, so nice.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = input_sentence.lower()\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a2d43",
   "metadata": {},
   "source": [
    "### Removing all characters that are not letters or spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e690a38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'during the summer we have the best ueather i have a black ueather jacket so nice'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Variable to replace all characters that are not letters or whitespace\n",
    "regex = re.compile('[^a-z\\s]')\n",
    "# Removes all characters that are not letters or spaces\n",
    "clean_text=regex.sub('', clean_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d5813",
   "metadata": {},
   "source": [
    "### Word Tokenization: Splitting the sentence into list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06687602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['during',\n",
       " 'the',\n",
       " 'summer',\n",
       " 'we',\n",
       " 'have',\n",
       " 'the',\n",
       " 'best',\n",
       " 'ueather',\n",
       " 'i',\n",
       " 'have',\n",
       " 'a',\n",
       " 'black',\n",
       " 'ueather',\n",
       " 'jacket',\n",
       " 'so',\n",
       " 'nice']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "input_words = word_tokenize(clean_text)\n",
    "input_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927e2ae",
   "metadata": {},
   "source": [
    "## Finding misspelt words and generating candidate words for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afdc432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(word):\n",
    "    # Generate candidate words for a misspelled word\n",
    "    candidates = set()\n",
    "    \n",
    "    # Insertion: Add a character at each position\n",
    "    for i in range(len(word) + 1):\n",
    "        for c in ascii_lowercase:\n",
    "            candidates.add(word[:i] + c + word[i:])\n",
    "    \n",
    "    # Deletion: Remove a character at each position\n",
    "    for i in range(len(word)):\n",
    "        candidates.add(word[:i] + word[i+1:])\n",
    "    \n",
    "    # Substitution: Replace each character with another character\n",
    "    for i in range(len(word)):\n",
    "        for c in ascii_lowercase:\n",
    "            candidates.add(word[:i] + c + word[i+1:])\n",
    "    \n",
    "    # Transposition: Swap adjacent characters\n",
    "    for i in range(len(word) - 1):\n",
    "        candidates.add(word[:i] + word[i+1] + word[i] + word[i+2:])\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def find_candidates(trie_root, sentence):\n",
    "    candidates = []\n",
    "    words = sentence.split()\n",
    "    \n",
    "    for word in words:\n",
    "        if not trie_root.is_prefix_exist(word):\n",
    "            # Generate candidates for misspelled word\n",
    "            word_candidates = generate_candidates(word)\n",
    "            # Check each candidate if it exists in the trie\n",
    "            for candidate in word_candidates:\n",
    "                if trie_root.is_prefix_exist(candidate):\n",
    "                    candidates.append(candidate)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "690dda93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: during\n",
      "Misspelled word's candidates: []\n",
      "\n",
      "Word: the\n",
      "Misspelled word's candidates: []\n",
      "\n",
      "Word: summer\n",
      "Misspelled word's candidates: []\n",
      "\n",
      "Word: we\n",
      "Misspelled word's candidates: []\n",
      "\n",
      "Word: have\n",
      "Misspelled word's candidates: []\n",
      "\n",
      "Word: the\n",
      "Misspelled word's candidates: []\n",
      "\n",
      "Word: best\n",
      "Misspelled word's candidates: []\n",
      "\n",
      "Word: ueather\n",
      "Misspelled word's candidates: ['teather', 'leather', 'feather', 'weather', 'heather', 'neather', 'yeather']\n",
      "\n",
      "Word: i\n",
      "Misspelled word's candidates: []\n",
      "\n",
      "Word: have\n",
      "Misspelled word's candidates: []\n",
      "\n",
      "Word: a\n",
      "Misspelled word's candidates: []\n",
      "\n",
      "Word: black\n",
      "Misspelled word's candidates: []\n",
      "\n",
      "Word: ueather\n",
      "Misspelled word's candidates: ['teather', 'leather', 'feather', 'weather', 'heather', 'neather', 'yeather']\n",
      "\n",
      "Word: jacket\n",
      "Misspelled word's candidates: []\n",
      "\n",
      "Word: so\n",
      "Misspelled word's candidates: []\n",
      "\n",
      "Word: nice\n",
      "Misspelled word's candidates: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from string import ascii_lowercase\n",
    "for word in input_words:\n",
    "    misspelled_candidates = find_candidates(root, word)\n",
    "    print(\"Word:\", word)\n",
    "    print(\"Misspelled word's candidates:\", misspelled_candidates)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c736d9e",
   "metadata": {},
   "source": [
    "## Using edit distance, we find the word to replace from the candidate list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0031d90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 teather\n",
      "1 leather\n",
      "1 feather\n",
      "1 weather\n",
      "1 heather\n",
      "1 neather\n",
      "1 yeather\n",
      "Misspelled word: ueather\n",
      "Correct word: teather\n",
      "1 teather\n",
      "1 leather\n",
      "1 feather\n",
      "1 weather\n",
      "1 heather\n",
      "1 neather\n",
      "1 yeather\n",
      "Misspelled word: ueather\n",
      "Correct word: teather\n"
     ]
    }
   ],
   "source": [
    "def edit_distance(word1, word2):\n",
    "    # Calculate the edit distance between two words using dynamic programming\n",
    "    m, n = len(word1), len(word2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    # Initialize the dp matrix\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # Calculate the edit distance\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if word1[i - 1] == word2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) + 1\n",
    "    \n",
    "    return dp[m][n]\n",
    "\n",
    "def choose_correct_word(original_word, candidates):\n",
    "    # Choose the correct word based on the context using minimum edit distance\n",
    "    min_distance = float('inf')\n",
    "    correct_word = None\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        distance = edit_distance(original_word, candidate)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            correct_word = candidate\n",
    "        print(min_distance,candidate)\n",
    "    return correct_word\n",
    "\n",
    "for word in input_words:\n",
    "    if not root.is_prefix_exist(word):\n",
    "        # Generate candidates for misspelled word\n",
    "        word_candidates = find_candidates(root, word)\n",
    "        # Choose the correct word based on the context\n",
    "        correct_word = choose_correct_word(word, word_candidates)\n",
    "        print(\"Misspelled word:\", word)\n",
    "        print(\"Correct word:\", correct_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82669259",
   "metadata": {},
   "outputs": [],
   "source": [
    "### above approach is not semantically correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23ca1a3",
   "metadata": {},
   "source": [
    "# Adding semantic value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81df5805",
   "metadata": {},
   "source": [
    "### Approach: Each candidate word is compared upon the semantic simalarity wrt to the given sentence and the highest valued candidate word is used to replace the missplelt words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e6fc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize text and preprocess\n",
    "tokenized_corpus = [word_tokenize(word.lower()) for word in words.words()]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save word embeddings to file\n",
    "word2vec_model.wv.save_word2vec_format('word2vec_embeddings.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4f9902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here, -0.0026996215 teather\n",
      "Here, -0.013643089 leather\n",
      "Here, -0.036639273 feather\n",
      "Here, -0.029646605 weather\n",
      "Here, -0.02142083 heather\n",
      "Here, 0.0 neather\n",
      "Here, 0.039927017 yeather\n",
      "Misspelled word: ueather\n",
      "Similar word: yeather\n",
      "Here, -0.0026996215 teather\n",
      "Here, -0.013643089 leather\n",
      "Here, -0.036639273 feather\n",
      "Here, -0.029646605 weather\n",
      "Here, -0.02142083 heather\n",
      "Here, 0.0 neather\n",
      "Here, 0.039927017 yeather\n",
      "Misspelled word: ueather\n",
      "Similar word: yeather\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained Word2Vec embeddings\n",
    "word_vectors = KeyedVectors.load_word2vec_format('word2vec_embeddings.bin', binary=True)\n",
    "\n",
    "def compute_sentence_semantic_similarity(sentence, candidate_word):\n",
    "    # Tokenize the input sentence\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    # Compute semantic similarity with each token in the sentence\n",
    "    similarities = []\n",
    "    for token in tokens:\n",
    "        if token in word_vectors and candidate_word in word_vectors:\n",
    "            similarity = word_vectors.similarity(token, candidate_word)\n",
    "            similarities.append(similarity)\n",
    "    \n",
    "    # Calculate the average similarity\n",
    "    if similarities:\n",
    "        avg_similarity = np.mean(similarities)\n",
    "    else:\n",
    "        avg_similarity = 0.0  # If no similarities found\n",
    "    \n",
    "    return avg_similarity\n",
    "\n",
    "'''# Example usage\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "candidate_word = \"cat\"\n",
    "semantic_similarity = compute_sentence_semantic_similarity(sentence, candidate_word)\n",
    "print(f\"Semantic similarity of '{candidate_word}' in the sentence: {semantic_similarity}\")\n",
    "'''\n",
    "\n",
    "for word in input_words:\n",
    "    if not root.is_prefix_exist(word):\n",
    "        # Generate candidates for misspelled word\n",
    "        word_candidates = find_candidates(root, word)\n",
    "        # Choose the correct word based on the context\n",
    "        most_similar=None\n",
    "        similarity_value=float('-inf')\n",
    "        for i in word_candidates:\n",
    "            semantic_similarity = compute_sentence_semantic_similarity(clean_text, i)\n",
    "            print('Here,',semantic_similarity,i)\n",
    "            if semantic_similarity>similarity_value:\n",
    "                similarity_value=semantic_similarity\n",
    "                most_similar=i\n",
    "\n",
    "        print(\"Misspelled word:\", word)\n",
    "        print(\"Similar word:\", most_similar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
